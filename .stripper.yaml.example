# Stripper Configuration File
# Copy this file to .stripper.yaml and modify as needed

# Crawler settings
crawler:
  # Maximum depth to crawl (default: 1)
  # - 1: Only crawl the initial URL
  # - 2: Crawl initial URL and all direct links
  # - 3+: Continue following links to specified depth
  depth: 2

  # Number of parallel workers (default: 4)
  # Controls how many pages can be processed simultaneously
  # - Higher values increase speed but also server load
  # - Lower values are gentler on the target server
  # - Recommended range: 2-8 depending on server capacity
  parallelism: 4

  # Output format: markdown, text, or html (default: markdown)
  # - markdown: Clean, formatted markdown content
  # - text: Plain text without formatting
  # - html: Original HTML content
  format: "markdown"

  # Output directory for crawled content (default: output)
  # Content will be organized by domain and URL path
  output_dir: "output"

  # File extensions to ignore during crawling
  # Add any extensions you want to skip
  ignore_extensions:
    # Documents
    - pdf
    - doc
    - docx
    - xls
    - xlsx

    # Images
    - jpg
    - jpeg
    - png
    - gif
    - ico

    # Web assets
    - css
    - js
    - woff
    - woff2
    - ttf
    - eot

    # Media
    - mp4
    - webm
    - mp3
    - wav

    # Archives
    - zip
    - tar
    - gz
    - rar

  # Rescan interval for previously crawled pages (e.g., 24h, 1h30m, 15m)
  # Format examples:
  # - 24h: 24 hours
  # - 1h30m: 1 hour and 30 minutes
  # - 15m: 15 minutes
  # Pages older than this will be recrawled to check for changes
  rescan_interval: "24h"

  # Reader API configuration
  reader_api:
    # Base URL for the Reader API (default: https://read.tabnot.space)
    # Change this if you're using a different Reader API endpoint
    url: "https://read.tabnot.space"

    # Additional headers to send with requests
    headers:
      # Response format matches the global format setting above
      X-Respond-With: "text"  # Can be text, markdown, or html

  # AI configuration for content summarization
  ai:
    # Enable AI summarization (default: false)
    enabled: false

    # OpenAI API endpoint (default: https://api.openai.com/v1)
    endpoint: "https://api.openai.com/v1"

    # Your OpenAI API key (required if AI is enabled)
    api_key: ""

    # Model to use for summarization (default: gpt-3.5-turbo)
    model: "gpt-3.5-turbo"

    # System prompt for AI summarization (required if AI is enabled)
    # This prompt guides how the AI should process and summarize the content
    system_prompt: |
      You are an intelligent assistant specialized in processing and extracting relevant information from web-scraped markdown or text documents. Your objective is to identify and extract key information while disregarding irrelevant or redundant content. The extracted data should be organized in a clear, structured, and consistent format.

      **Instructions:**

      1. **Extract Metadata:**
         - **URL:** The source URL of the document.
         - **Date:** The date and time when the content was scraped or published (if available).

      2. **Identify and Extract Titles and Headings:**
         - **Main Title:** The primary title of the document.
         - **Subheadings:** Significant subheadings indicating important sections.

      3. **Extract Images:**
         - For each image, capture:
           - **Alt Text or Description:** Text describing the image.
           - **Image URL:** The direct link to the image.

      4. **Extract Links:**
         - For each hyperlink, capture:
           - **Display Text:** The visible text of the link.
           - **Destination URL:** The URL the link points to.

      5. **Extract Contact Information:**
         - Identify and extract any contact details such as:
           - **Email Addresses**
           - **Phone Numbers**
           - **Physical Addresses**
           - **Contact Forms or Feedback Links**

      6. **Extract Resource Sections:**
         - Identify sections commonly labeled as "Resources," "References," "Downloads," etc.

      7. **Extract Lists and Tables:**
         - Capture any bulleted or numbered lists that contain significant information.
         - Extract data from tables, maintaining headers and corresponding data rows.

      8. **Organize in Markdown Format:**
         - Present the information using appropriate headings, lists, and formatting.

      9. **Exclude:**
         - Navigation menus, headers, footers
         - Boilerplate text and generic content
         - Scripts and styles
         - Advertisements

# HTTP client settings
http:
  # Request timeout in seconds
  timeout: 30

  # Number of retry attempts for failed requests
  retry_attempts: 3

  # Delay between retries in seconds
  retry_delay: 5

  # User agent string for requests
  user_agent: "Stripper/1.0 Web Content Crawler"

  # Delay between requests in milliseconds (to be nice to servers)
  request_delay: 1000
